{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describe: Creating 'cloze' exercises with Cicero"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Devise\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan\n",
    "\n",
    "As always, let's plan out our work before we start writing Python code. We will use the following steps to create our cloze exercises:\n",
    "\n",
    "**Pseudocode for Cicerorian 'cloze' exercises**\n",
    "\n",
    "- Load our library of Latin texts, keeping only those by Cicero\n",
    "- Create a list of sentences from which we can draw our exercises, keeping them at a certain length (~10-25 words)\n",
    "- Pick a sentence at random\n",
    "- Pick a word at random to mask\n",
    "- Create a set of multiple-choice answers, i.e. three random words in addition to the removed word\n",
    "- Ask user for input to test whether the removed word can be correctly identified"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminary imports\n",
    "from natsort import natsorted\n",
    "from pprint import pprint\n",
    "from time import sleep"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, let's set up our corpus reader and pull out the texts we want to describe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PC 1: Load our library of Latin texts, keeping only those by Cicero\n",
    "\n",
    "from cltkreaders.lat import LatinTesseraeCorpusReader\n",
    "\n",
    "T = LatinTesseraeCorpusReader()\n",
    "\n",
    "cicero = natsorted([fileid for fileid in T.fileids() if 'de_finibus' in fileid])\n",
    "pprint(cicero[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PC 2a: Create a list of sentences from which we can draw our exercises\n",
    "\n",
    "sents = list(T.sents(fileids=cicero))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example sentences\n",
    "\n",
    "for i, sent in enumerate(sents[:10], 1):\n",
    "    print(f'{i}: {sent}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [sent.as_doc() for sent in sents if len(sent) > 10 and len(sent) < 25]\n",
    "for i, sent in enumerate(sents[:10], 1):\n",
    "    print(f'{i}: {sent}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process of loading these sentences into memory takes 15 seconds on my machine. To load all of the sentences from the Cicero files would take even longer. When we find outselves in a situation like this it can often be a huge timesaver to write these kinds of computation-intensive results to disk for quick retrieval later. Here is an example of \"pickling\" the sentences we just loaded so they can be loaded from disk as opposed to reprocessed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "sents = [sent for sent in sents] # Convert to strings\n",
    "pickle.dump(sents, open('../data/cicero-sents.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = pickle.load(open('../data/cicero-sents.pickle', 'rb'))\n",
    "sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PC 3: Pick a sentence at random\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "exercise = random.choice(sents)\n",
    "exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PC 4: Pick a word at random to mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, token in enumerate(exercise):\n",
    "    print(f'{i}: {token}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in exercise:\n",
    "    print(f'{token.i}: {token.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in exercise:\n",
    "    print(f'{token.i}: {token.is_alpha}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_options = [token.i for token in exercise if token.is_alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "remove_choice = random.choice(remove_options)\n",
    "remove_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloze = ' '.join([token.text if token.i != remove_choice else '_____' for token in exercise])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = exercise[remove_choice].text\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PC 5: Create a set of multiple-choice answers, i.e. three random words in addition to the removed word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set([word.text for sent in sents for word in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "wrong_answers = random.sample(list(vocab - {answer}), 3)\n",
    "wrong_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PC 6: Ask user for input to test whether the removed word can be correctly identified\n",
    "\n",
    "quiz = {cloze: [answer] + wrong_answers}\n",
    "pprint(quiz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question, alternatives in quiz.items():\n",
    "    correct_answer = alternatives[0]\n",
    "    for alternative in sorted(alternatives):\n",
    "        print(f\"  - {alternative}\")\n",
    "    print()\n",
    "    \n",
    "    answer = input(f\"{question}? \")\n",
    "    if answer == correct_answer:\n",
    "        print(\"Correct!\")\n",
    "    else:\n",
    "        print(f\"Incorrect! The answer is {correct_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cloze_qa_bank(sents, vocab, n=10):\n",
    "    sents = random.sample(sents, n)\n",
    "    cloze_qa_bank = {}\n",
    "    for sent in sents:\n",
    "        remove_options = [token.i for token in sent if token.is_alpha]\n",
    "        remove_choice = random.choice(remove_options)\n",
    "        cloze = ' '.join([token.text if token.i != remove_choice else '_____' for token in sent])\n",
    "        answer = sent[remove_choice].text\n",
    "        wrong_answers = random.sample(list(vocab - {answer}), 3)\n",
    "        cloze_qa_bank[cloze] = [answer] + wrong_answers\n",
    "    return cloze_qa_bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quiz = create_cloze_qa_bank(sents, vocab, n=10)\n",
    "\n",
    "# for question, alternatives in quiz.items():\n",
    "#     correct_answer = alternatives[0]\n",
    "#     for alternative in sorted(alternatives):\n",
    "#         print(f\"  - {alternative}\")\n",
    "#     print()\n",
    "    \n",
    "#     answer = input(f\"{question}? \")\n",
    "#     if answer == correct_answer:\n",
    "#         print(\"Correct!\")\n",
    "#     else:\n",
    "#         print(f\"Incorrect! The answer is {correct_answer}\")\n",
    "#     print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "\n",
    "- ***Change author***: It is becoming a pattern! But that is because this is where exploration lies for us, at least in the early stages. Experiment with sentences from authors other than Cicero, or the works of Cicero that we have not yet looked at. \n",
    "- ***Change objective***: Try inserting a random word into a sentence and seeing if the user can identify the errant addition. Try scrambling the letters of one or more words (all?) in a sentence. Get the part of speech of masked words and ask the user for madlib style insertions. This is a Deform experiment—feel free to manipulate the text in any way you see fit. Claassen 1991 recommends an exercise where \"the program omits at random intervals the last two letters of any word... [and] the students must complete the blanks.\" How would you implement this?\n",
    "\n",
    "### For the future\n",
    "\n",
    "- ***Consider the multiple choice***: Right now we are inserting random words from Cicero's vocabulary into the multiple choice. There are more principled ways of going about this process though. It may already have been clear from previous examples that choosing words randomly for the vocbulary produces some pretty unlikely candidates for filling-in-the-blank. Consider how we would address this? One idea would be to only return words with the same part of speech. Another idea—this one cribbed from Duolingo's language courses—would be to build a list of words with similar but not exactly the same spelling, e.g. *manet* for *monet*; you can read up on the idea of \"edit distance\" as an entry point into this approach. But the real payoff is going to be in using vector semantics or word embedding models. Word embeddings are a numerical representation of lexical items and specifically dense vector representation. And since they are numerical representations these word vectors can be compared for similarity. Look at Notebok 8a for a quick tour of how vectors can be used for such a task."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "- Claassen, J.-M. 1991. “The Design of Computer Software for Learning Latin.” *Per Linguam* 7(1): 3–23."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit ('exploratory-philology')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "90f29fce041a746dff5b4e7dc43fcbd6facb181eb5d3b96918a0244a9c00c3c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to CLTK Readers\n",
    "\n",
    "CLTK Readers is a corpus reader extension written for use with the Classical Language Toolkit, LatinCy, etc. It gives single-line api access to philologically sensible units (i.e. words, sentences, paragraph, documents, etc.) for basic processing and analyses of Latin text collections. Some readers have extended, higher-order functionality, such as the concordancer for the CLTK Tesserae texts as shown below. The repository for CLTK readers can be found [here](https://github.com/diyclassics/cltk_readers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "\n",
    "from cltkreaders.lat import LatinTesseraeCorpusReader\n",
    "from os.path import expanduser\n",
    "from natsort import natsorted\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up reader\n",
    "# NB: If you do not have the CLTK-Tesserae corpus already installed in CLTK_DATA, you will be prompted to download the corpus.\n",
    "\n",
    "T = LatinTesseraeCorpusReader()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fileids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ammianus.rerum_gestarum.part.14.tess',\n",
      " 'ammianus.rerum_gestarum.part.15.tess',\n",
      " 'ammianus.rerum_gestarum.part.16.tess',\n",
      " 'ammianus.rerum_gestarum.part.17.tess',\n",
      " 'ammianus.rerum_gestarum.part.18.tess',\n",
      " 'ammianus.rerum_gestarum.part.19.tess',\n",
      " 'ammianus.rerum_gestarum.part.20.tess',\n",
      " 'ammianus.rerum_gestarum.part.21.tess',\n",
      " 'ammianus.rerum_gestarum.part.22.tess',\n",
      " 'ammianus.rerum_gestarum.part.23.tess']\n"
     ]
    }
   ],
   "source": [
    "## First 10 filesnames\n",
    "\n",
    "pprint(T.fileids()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cicero.academica.tess',\n",
      " 'cicero.brutus.tess',\n",
      " 'cicero.cum_populo_gratias_egit.tess',\n",
      " 'cicero.de_amicitia.tess',\n",
      " 'cicero.de_divinatione.tess',\n",
      " 'cicero.de_domo_sua.tess',\n",
      " 'cicero.de_fato.tess',\n",
      " 'cicero.de_finibus_bonorum_et_malorum.part.1.tess',\n",
      " 'cicero.de_finibus_bonorum_et_malorum.part.2.tess',\n",
      " 'cicero.de_finibus_bonorum_et_malorum.part.3.tess']\n"
     ]
    }
   ],
   "source": [
    "## First 10 works of Cicero\n",
    "\n",
    "cicero = [file for file in T.fileids() if 'cicero' in file]\n",
    "pprint(cicero[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vergil.aeneid.part.1.tess',\n",
      " 'vergil.aeneid.part.2.tess',\n",
      " 'vergil.aeneid.part.3.tess',\n",
      " 'vergil.aeneid.part.4.tess',\n",
      " 'vergil.aeneid.part.5.tess',\n",
      " 'vergil.aeneid.part.6.tess',\n",
      " 'vergil.aeneid.part.7.tess',\n",
      " 'vergil.aeneid.part.8.tess',\n",
      " 'vergil.aeneid.part.9.tess',\n",
      " 'vergil.aeneid.part.10.tess',\n",
      " 'vergil.aeneid.part.11.tess',\n",
      " 'vergil.aeneid.part.12.tess']\n"
     ]
    }
   ],
   "source": [
    "## Books of the Aeneid, sorted\n",
    "\n",
    "aeneid = natsorted([file for file in T.fileids() if 'aeneid' in file])\n",
    "pprint(aeneid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "catullus = 'catullus.carmina.tess'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<cat. 1.1>\tCui dono lepidum novum libellum\n",
      "<cat. 1.2>\tarido modo pumice expolitum?\n",
      "<cat. 1.3>\tCorneli, tibi; namque tu solebas\n",
      "<cat. 1.4>\tmeas esse aliquid putare nugas,\n",
      "<cat. 1.5>\tiam tum cum ausus es unus Italorum\n",
      "<cat. 1.6>\tomne aevum tribus explicare chartis,\n",
      "<cat. 1.7>\tdoctis, Iuppiter, et laboriosis!\n",
      "<cat. 1.8>\tquare habe tibi quidquid hoc libelli\n",
      "<cat. 1.9>\tqualecumque, quod, o patrona virgo,\n",
      "<cat. 1.10>\tplus uno maneat perenne saeclo.\n"
     ]
    }
   ],
   "source": [
    "## Docs\n",
    "\n",
    "catullus_doc = T.docs(catullus)\n",
    "print(next(catullus_doc)[:446])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cui dono lepidum novum libellum\n",
      "arido modo pumice expolitum?\n",
      "Corneli, tibi; namque tu solebas\n",
      "meas esse aliquid putare nugas,\n",
      "iam tum cum ausus es unus Italorum\n",
      "omne aevum tribus explicare chartis,\n",
      "doctis, Iuppiter, et laboriosis!\n",
      "quare habe tibi quidquid hoc libelli\n",
      "qualecumque, quod, o patrona virgo,\n",
      "plus uno maneat perenne saeclo.\n"
     ]
    }
   ],
   "source": [
    "## Texts\n",
    "\n",
    "catullus_text = T.texts(catullus)\n",
    "print(next(catullus_text)[:335])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a string representation of what the output dictionary looks like...\n",
      "{'<cat. 1.1>': 'Cui dono lepidum novum libellum', '<cat. 1.2>': 'arido modo pumice expolitum?' etc. }\n",
      "\n",
      "Here are the first 10 items of the dict output...\n",
      "[('<cat. 1.1>', 'Cui dono lepidum novum libellum'),\n",
      " ('<cat. 1.2>', 'arido modo pumice expolitum?'),\n",
      " ('<cat. 1.3>', 'Corneli, tibi; namque tu solebas'),\n",
      " ('<cat. 1.4>', 'meas esse aliquid putare nugas,'),\n",
      " ('<cat. 1.5>', 'iam tum cum ausus es unus Italorum'),\n",
      " ('<cat. 1.6>', 'omne aevum tribus explicare chartis,'),\n",
      " ('<cat. 1.7>', 'doctis, Iuppiter, et laboriosis!'),\n",
      " ('<cat. 1.8>', 'quare habe tibi quidquid hoc libelli'),\n",
      " ('<cat. 1.9>', 'qualecumque, quod, o patrona virgo,'),\n",
      " ('<cat. 1.10>', 'plus uno maneat perenne saeclo.')]\n"
     ]
    }
   ],
   "source": [
    "## Doc Rows\n",
    "\n",
    "catullus_docrows = T.doc_rows(catullus)\n",
    "\n",
    "print('This is a string representation of what the output dictionary looks like...')\n",
    "print(f'{str(next(catullus_docrows))[:94]} etc. }}\\n')\n",
    "\n",
    "\n",
    "catullus_docrows = T.doc_rows(catullus)\n",
    "print('Here are the first 10 items of the dict output...')\n",
    "pprint(list(next(catullus_docrows).items())[:10])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "catilinam = 'cicero.in_catilinam.tess'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note that for the Tesserae texts, `paras` are *not* implemented. As they are not consistently marked in the original files.\n"
     ]
    }
   ],
   "source": [
    "## Paras\n",
    "\n",
    "print(\"Note that for the Tesserae texts, `paras` are *not* implemented. As they are not consistently marked in the original files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent 1: quo usque tandem abutere, Catilina, patientia nostra?\n",
      "Sent 2: quam diu etiam furor iste tuus nos eludet?\n",
      "Sent 3: quem ad finem sese effrenata iactabit audacia?\n",
      "Sent 4: nihilne te nocturnum praesidium Palati, nihil urbis vigiliae, nihil timor populi, nihil concursus bonorum omnium, nihil hic munitissimus habendi senatus locus, nihil horum ora voltusque moverunt?\n",
      "Sent 5: patere tua consilia non sentis, constrictam iam horum omnium scientia teneri coniurationem tuam non vides?\n"
     ]
    }
   ],
   "source": [
    "# Sents\n",
    "\n",
    "# NB: Sents are segmented by default with the CLTK LatinPunktSentenceTokenizer\n",
    "\n",
    "catilinam_sents = T.sents(catilinam)\n",
    "\n",
    "for i in range(1,6):\n",
    "    print(f'Sent {i}: {next(catilinam_sents)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 1: quo\n",
      "Word 2: usque\n",
      "Word 3: tandem\n",
      "Word 4: abutere\n",
      "Word 5: ,\n",
      "Word 6: Catilina\n",
      "Word 7: ,\n",
      "Word 8: patientia\n",
      "Word 9: nostra\n"
     ]
    }
   ],
   "source": [
    "# Words\n",
    "\n",
    "# NB: Words are tokenized by default with the CLTK LatinWordTokenizer\n",
    "\n",
    "catilinam_words = T.words(catilinam)\n",
    "\n",
    "for i in range(1,10):\n",
    "    print(f'Word {i}: {next(catilinam_words)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 1: quo\n",
      "Word 2: usque\n",
      "Word 3: tandem\n",
      "Word 4: abutere\n",
      "Word 5: ,\n",
      "Word 6: catilina\n",
      "Word 7: ,\n",
      "Word 8: patientia\n",
      "Word 9: nostra\n"
     ]
    }
   ],
   "source": [
    "# You can pass a preprocessor to `words` \n",
    "\n",
    "def custom_preprocess(text):\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "catilinam_words = T.words(catilinam, preprocess=custom_preprocess)\n",
    "\n",
    "for i in range(1,10):\n",
    "    print(f'Word {i}: {next(catilinam_words)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tok Sent 1: [('quo', 'qui', 'PRON'), ('usque', 'usque', 'ADV'), ('tandem', 'tandem', 'ADV'), ('abutere', 'abutor', 'VERB'), (',', ',', 'PUNCT'), ('Catilina', 'Catilina', 'NOUN'), (',', ',', 'PUNCT'), ('patientia', 'patientia', 'NOUN'), ('nostra', 'noster', 'ADJ'), ('?', '?', 'PUNCT')]\n",
      "Tok Sent 2: [('quam', 'qui', 'PRON'), ('diu', 'diu', 'ADV'), ('etiam', 'etiam', 'ADV'), ('furor', 'furor', 'NOUN'), ('iste', 'iste', 'DET'), ('tuus', 'tuus', 'ADJ'), ('nos', 'nos', 'PRON'), ('eludet', 'eludo', 'VERB'), ('?', '?', 'PUNCT')]\n",
      "Tok Sent 3: [('quem', 'qui', 'PRON'), ('ad', 'ad', 'ADP'), ('finem', 'finis', 'NOUN'), ('sese', 'se', 'PRON'), ('effrenata', 'effreno', 'VERB'), ('iactabit', 'iacto', 'VERB'), ('audacia', 'audacia', 'NOUN'), ('?', '?', 'PUNCT')]\n",
      "Tok Sent 4: [('nihilne', 'nihilne', 'ADJ'), ('te', 'tu', 'PRON'), ('nocturnum', 'nocturnus', 'ADJ'), ('praesidium', 'praesidium', 'NOUN'), ('Palati', 'Palati', 'NOUN'), (',', ',', 'PUNCT'), ('nihil', 'nihil', ''), ('urbis', 'urbs', 'NOUN'), ('vigiliae', 'uigilia', 'NOUN'), (',', ',', 'PUNCT'), ('nihil', 'nihil', ''), ('timor', 'timor', 'NOUN'), ('populi', 'populus', 'NOUN'), (',', ',', 'PUNCT'), ('nihil', 'nihil', ''), ('concursus', 'concursus', 'NOUN'), ('bonorum', 'bonus', 'ADJ'), ('omnium', 'omnis', 'ADJ'), (',', ',', 'PUNCT'), ('nihil', 'nihil', ''), ('hic', 'hic', 'DET'), ('munitissimus', 'munitus', 'ADJ'), ('habendi', 'habeo', 'VERB'), ('senatus', 'senatus', 'NOUN'), ('locus', 'locus', 'NOUN'), (',', ',', 'PUNCT'), ('nihil', 'nihil', ''), ('horum', 'hic', 'DET'), ('ora', 'os', 'NOUN'), ('voltus', 'uoltus', 'NOUN'), ('que', 'que', 'CCONJ'), ('moverunt', 'moverunt', 'VERB'), ('?', '?', 'PUNCT')]\n",
      "Tok Sent 5: [('patere', 'pateo', 'VERB'), ('tua', 'tuus', 'ADJ'), ('consilia', 'consilium', 'NOUN'), ('non', 'non', 'PART'), ('sentis', 'sentio', 'VERB'), (',', ',', 'PUNCT'), ('constrictam', 'constringo', 'NOUN'), ('iam', 'iam', 'ADV'), ('horum', 'hic', 'DET'), ('omnium', 'omnis', 'ADJ'), ('scientia', 'scientia', 'NOUN'), ('teneri', 'teneo', 'VERB'), ('coniurationem', 'coniuratio', 'NOUN'), ('tuam', 'tuus', 'ADJ'), ('non', 'non', 'PART'), ('vides', 'uideo', 'VERB'), ('?', '?', 'PUNCT')]\n",
      "Tok Sent 6: [('quid', 'quis', 'PRON'), ('proxima', 'proximus', 'ADJ'), (',', ',', 'PUNCT'), ('quid', 'quis', 'PRON'), ('superiore', 'superus', 'ADJ'), ('nocte', 'nox', 'NOUN'), ('egeris', 'egeris', 'VERB'), (',', ',', 'PUNCT'), ('ubi', 'ubi', 'ADV'), ('fueris', 'sum', 'AUX'), (',', ',', 'PUNCT'), ('quos', 'qui', 'PRON'), ('convocaveris', 'convoco', 'VERB'), (',', ',', 'PUNCT'), ('quid', 'quis', 'PRON'), ('consili', 'consilium', 'NOUN'), ('ceperis', 'ceperis', 'VERB'), ('quem', 'qui', 'PRON'), ('nostrum', 'nos', 'ADJ'), ('ignorare', 'ignoro', 'VERB'), ('arbitraris', 'arbitro', 'VERB'), ('?', '?', 'PUNCT')]\n",
      "Tok Sent 7: [('O', 'o', 'INTJ'), ('tempora', 'tempus', 'NOUN'), (',', ',', 'PUNCT'), ('o', 'o', 'INTJ'), ('mores', 'mos', 'NOUN'), ('!', '!', 'PUNCT'), ('senatus', 'senatus', 'NOUN'), ('haec', 'hic', 'DET'), ('intellegit', 'intellego', 'VERB'), (',', ',', 'PUNCT'), ('consul', 'consul', 'NOUN'), ('videt', 'uideo', 'VERB'), (';', ';', 'PUNCT')]\n",
      "Tok Sent 8: [('hic', 'hic', 'DET'), ('tamen', 'tamen', 'ADV'), ('vivit', 'uiuo', 'VERB'), ('.', '.', 'PUNCT')]\n",
      "Tok Sent 9: [('vivit', 'uiuo', 'VERB'), ('?', '?', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "# Tokenized sents\n",
    "\n",
    "# A combination of the two structures above; convenient for many applications that require lists of tokenized sentences\n",
    "\n",
    "catilinam_tokenized_sents = T.tokenized_sents(catilinam)\n",
    "\n",
    "for i in range(1,10):\n",
    "    print(f'Tok Sent {i}: {next(catilinam_tokenized_sents)}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "metamorphoses = natsorted([file for file in T.fileids() if 'ovid.metamorphoses' in file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_preprocess(text):\n",
    "    from cltk.alphabet.lat import JVReplacer\n",
    "    replacer = JVReplacer()\n",
    "\n",
    "    text = text.lower() # Lowercase\n",
    "    text = replacer.replace(text)  # Normalize u/v & i/j\n",
    "\n",
    "    # Remove punctuation\n",
    "    punctuation =\"\\\"#$%&\\'()*+,/:;<=>@[\\]^_`{|}~.?!«»—“-”\"\n",
    "    misc = '¡£¤¥¦§¨©¯°±²³´µ¶·¸¹º¼½¾¿÷·–‘’†•ↄ∞⏑〈〉（）'\n",
    "    misc += punctuation\n",
    "    translator = str.maketrans({key: \" \" for key in misc})\n",
    "    text = text.translate(translator)\n",
    "\n",
    "    # Remove numbers\n",
    "    translator = str.maketrans({key: \" \" for key in '0123456789'})\n",
    "    text = text.translate(translator)\n",
    "\n",
    "    return \" \".join(text.split()).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', [('<ov. met. 1.145>', 2), ('<ov. met. 1.587>', 1)]),\n",
      " ('ab',\n",
      "  [('<ov. met. 1.3>', 3),\n",
      "   ('<ov. met. 1.23>', 4),\n",
      "   ('<ov. met. 1.34>', 5),\n",
      "   ('<ov. met. 1.40>', 5),\n",
      "   ('<ov. met. 1.66>', 4),\n",
      "   ('<ov. met. 1.80>', 5),\n",
      "   ('<ov. met. 1.144>', 5),\n",
      "   ('<ov. met. 1.185>', 7),\n",
      "   ('<ov. met. 1.233>', 4),\n",
      "   ('<ov. met. 1.254>', 6),\n",
      "   ('<ov. met. 1.269>', 5),\n",
      "   ('<ov. met. 1.313>', 4),\n",
      "   ('<ov. met. 1.336>', 6),\n",
      "   ('<ov. met. 1.417>', 6),\n",
      "   ('<ov. met. 1.431>', 2),\n",
      "   ('<ov. met. 1.568>', 6),\n",
      "   ('<ov. met. 1.607>', 5),\n",
      "   ('<ov. met. 1.672>', 6),\n",
      "   ('<ov. met. 1.774>', 7)]),\n",
      " ('aberant', [('<ov. met. 1.91>', 2)])]\n"
     ]
    }
   ],
   "source": [
    "## Concordance, using Tesserae citations\n",
    "\n",
    "metamorphoses_concordances = T.concordance(metamorphoses, preprocess=custom_preprocess)\n",
    "\n",
    "met_conc_sample = next(metamorphoses_concordances)\n",
    "pprint(list(met_conc_sample.items())[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('accensis', [('<ov. met. 12.12>', 2)]),\n",
      " ('accensum', [('<ov. met. 2.228>', 1)]),\n",
      " ('accensus', [('<ov. met. 11.527>', 4)]),\n",
      " ('acceperat', [('<ov. met. 3.121>', 5)]),\n",
      " ('accepere', [('<ov. met. 9.719>', 0), ('<ov. met. 15.641>', 4)]),\n",
      " ('accepisse',\n",
      "  [('<ov. met. 6.357>', 0),\n",
      "   ('<ov. met. 14.844>', 4),\n",
      "   ('<ov. met. 15.481>', 0)])]\n"
     ]
    }
   ],
   "source": [
    "# Concordances are by default built on a file-by-file basis, but can easily be combined with the `compiled` parameter\n",
    "\n",
    "metamorphoses_concordances = T.concordance(metamorphoses, compiled=True, preprocess=custom_preprocess)\n",
    "\n",
    "full_met_conc_sample = next(metamorphoses_concordances)\n",
    "pprint(list(full_met_conc_sample.items())[96:102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'corpus' appears 67 times in the Metamorphoses.\n",
      "Here are the first five instances...\n",
      "[('<ov. met. 1.190>', 5), ('<ov. met. 2.362>', 6), ('<ov. met. 2.611>', 0), ('<ov. met. 2.647>', 2), ('<ov. met. 2.648>', 2)]\n",
      "\n",
      "'corpora' appears 86 times in the Metamorphoses.\n",
      "Here are the first five instances...\n",
      "[('<ov. met. 1.2>', 0), ('<ov. met. 1.156>', 4), ('<ov. met. 1.300>', 5), ('<ov. met. 1.527>', 5), ('<ov. met. 2.235>', 4)]\n"
     ]
    }
   ],
   "source": [
    "# Since the concordances are output as dictionaries, you can retrieve location information using the token as a dict key...\n",
    "\n",
    "metamorphoses_concordances = T.concordance(metamorphoses, compiled=True, preprocess=custom_preprocess)\n",
    "full_met_conc_sample = next(metamorphoses_concordances)\n",
    "\n",
    "print(f'\\'corpus\\' appears {len(full_met_conc_sample[\"corpus\"])} times in the Metamorphoses.')\n",
    "print('Here are the first five instances...')\n",
    "print(full_met_conc_sample['corpus'][:5])\n",
    "\n",
    "print()\n",
    "\n",
    "print(f'\\'corpora\\' appears {len(full_met_conc_sample[\"corpora\"])} times in the Metamorphoses.')\n",
    "print('Here are the first five instances...')\n",
    "print(full_met_conc_sample['corpora'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats on just the file 'catullus.carmina.tess'\n",
      "{'files': 1,\n",
      " 'lexdiv': 2.676677535610091,\n",
      " 'secs': 7.7393670082092285,\n",
      " 'sents': 822,\n",
      " 'vocab': 5827,\n",
      " 'words': 15597}\n"
     ]
    }
   ],
   "source": [
    "## Basic descriptive data; this data can also be returned for individual files or lists of files\n",
    "# Here just Catullus...\n",
    "\n",
    "print('Stats on just the file \\'catullus.carmina.tess\\'')\n",
    "pprint(T.describe(catullus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats on just the group of files assigned above to the variable `metamorphoses`\n",
      "{'files': 15,\n",
      " 'lexdiv': 5.033443456162643,\n",
      " 'secs': 49.04771399497986,\n",
      " 'sents': 5372,\n",
      " 'vocab': 19675,\n",
      " 'words': 99033}\n"
     ]
    }
   ],
   "source": [
    "print('Stats on just the group of files assigned above to the variable `metamorphoses`')\n",
    "pprint(T.describe(metamorphoses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Basic descriptive data; note takes several minutes to run\n",
    "\n",
    "# tess_describe = T.describe()\n",
    "# pprint(tess_describe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('stanford')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f28f2655caa070e39b75c186f98b8f52da1af34bdb8dab0b58a93e2439370a2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divert: Making word search puzzles from Catullus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Devise\n",
    "Not all pedagogy needs to be exercises, drills, quizzes, tests, etc. We are allowed to have fun as well. We can find no shortage of Latin crossword puzzles—the [*Guardian*](https://www.theguardian.com/crosswords/crossword-blog/2015/oct/19/crossword-blog-return-latin) published one in 1930 and restarted (and at some point restopped?) in 2015. Latin word games more generally can be found here and there online, as for example, [*Hebdomada Aenigmatum*](https://www.latincrosswords.com/). We even have [Latin Wordle](https://wordle.latindictionary.io/) and [Latin Spelling Bee](https://www.examenapium.com/) to keep ourselves entertained as we learn. Speaking from my own experience as a young child, word puzzle magazines from *Dell* and *Pennypress* were a source of endless engagement with language well before I knew exactly what every word meant or could even fill out 10% of a crossword puzzle grid.\n",
    "\n",
    "So, it is in the *Exploratory Philology* spirit to leverage computational methods toward any activity that brings us closer to that kind of \"endless engagement\". The subtitle of the book is \"learning about Ancient Greek and Latin\" and even in a simple word search puzzle we are inevitably learning *about* the lanaguage: we are learning about letter patterns, letter frequencies, character cluster frequencies, maybe even something, as we will see below, about lexical categories and key words.\n",
    "\n",
    "In this notebook, we will build word search puzzles from the works of Catullus and at the end of this activity you will have a letter grid with hidden words that you can distribute to your students for a quick and fun classroom activity. But as we will see, the process of constructing the puzzle is itself a learning experience. In a certain respect, we can think of it as a re-learning experience, since we will be drawing on Python skills from the previous three notebooks and directing them toward a new end."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan\n",
    "\n",
    "What do we need to do to make a word search puzzle from a given poem of Catullus? There are really two separate tasks that we need to consider here: 1. the technical task of creating a word search puzzle from any text and 2. the more philological task of selecting the best words from any given poem to use for our wordlist. The first task we will largely leave to an existing Python module—note that this in itself is a useful lesson in computer programming: there is no for wheel-reinventing especially when reusing and adapting existing code allows us to focus more on what we are really interested in, namely learning about Latin texts through code. The second task gives us an opportunity to introduce some more text analysis fundamentals, specifically the idea of keyness. As opposed to our earlier Describe task which was genuinely about frequency counts, here we turn our attention not simply to which words appear most often but rather which ones play an important role in the text we are looking at. To foreshadow our work in this experiment, if we are making a word search puzzle for Catullus 2, how do we make sure that *passer* is in our wordlist?\n",
    "\n",
    "Our two-part pseudocode...\n",
    "\n",
    "**Pseudocode for making word search puzzles for Catullus**\n",
    "\n",
    "- Task 1\n",
    "    - Import an off-the-shelf solution for making word search puzzles\n",
    "- Task 2\n",
    "    - Load our library of Latin texts, keeping only the poems of Catullus\n",
    "    - Define a \"keyness\" measure for our poems, here TF-IDF\n",
    "    - Measure keyness for specific poem and select some number of words for our wordlist\n",
    "    - Make a word search puzzle from our wordlist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminary imports\n",
    "from natsort import natsorted\n",
    "from pprint import pprint\n",
    "import random\n",
    "from time import sleep\n",
    "from latintools import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PC 1: Load our library of Latin texts, keeping only Catullus\n",
    "\n",
    "from cltkreaders.lat import LatinTesseraeCorpusReader\n",
    "\n",
    "T = LatinTesseraeCorpusReader()\n",
    "\n",
    "catullus = [fileid for fileid in T.fileids() if 'catullus' in fileid][0] # There is only one Catullus file\n",
    "print(catullus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "docrows = next(T.doc_rows(catullus))\n",
    "\n",
    "df = pd.DataFrame.from_dict(docrows, orient='index', columns=['line'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the dataframe\n",
    "df = pd.DataFrame.from_dict(docrows, orient='index', columns=['line'])\n",
    "df['author'] = 'catullus'\n",
    "df['poem'] = df.apply(lambda row: row.name.split('.')[1].replace('>','').strip(), axis=1)\n",
    "df['line_no'] = df.apply(lambda row: row.name.split('.')[2].replace('>','').strip(), axis=1)\n",
    "df['line'] = df.apply(lambda row: row.line.replace('\\n','').strip(), axis=1)\n",
    "df = df[['author', 'poem', 'line_no', 'line']]\n",
    "df = df.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['poem'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['poem'] == '2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem = \"\\n\".join(df[df['poem'] == '2']['line'].tolist())\n",
    "print(poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_poem(df, poem_no):\n",
    "    poem = \"\\n\".join(df[df['poem'] == str(poem_no)]['line'].tolist())\n",
    "    return poem\n",
    "\n",
    "catullus_poems = {}\n",
    "\n",
    "for poem_no in df['poem'].unique():\n",
    "    poem = make_poem(df, poem_no)\n",
    "    catullus_poems[poem_no] = poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(list(catullus_poems.items())[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catullus_poems['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catullus_preprocess = {no: preprocess(poem, remove_lines=True) for no, poem in catullus_poems.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(catullus_preprocess['1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "\n",
    "def tf(word, blob):\n",
    "    return blob.words.count(word) / len(blob.words)\n",
    "\n",
    "def n_containing(word, bloblist):\n",
    "    return sum(1 for blob in bloblist if word in blob.words)\n",
    "\n",
    "def idf(word, bloblist):\n",
    "    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))\n",
    "\n",
    "def tfidf(word, blob, bloblist):\n",
    "    return tf(word, blob) * idf(word, bloblist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catullus_preprocess_vals = [preprocess(poem) for poem in catullus_poems.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bloblist = [tb(poem) for poem in catullus_preprocess_vals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [{word: tfidf(word, blob, bloblist) for word in blob.words} for blob in bloblist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = sorted(list(scores[2].items()), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "print(tabulate(test, headers=['Word', 'Score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_poems = [list(blob.words) for blob in bloblist]\n",
    "vocab = set([word for poem in vocab_poems for word in poem])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_df = {}\n",
    "\n",
    "for poem in vocab_poems:\n",
    "    for word in set(poem):\n",
    "        if word in vocab_df:\n",
    "            vocab_df[word] += 1\n",
    "        else:\n",
    "            vocab_df[word] = 1\n",
    "\n",
    "vocab_df = sorted(vocab_df.items(), key=lambda x: x[1], reverse=True)\n",
    "vocab_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter([word for poem in vocab_poems for word in poem]).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = sorted(list(scores[2].items()), key=lambda x: x[1], reverse=True)\n",
    "pprint(test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist_base = [word for word, score in test]\n",
    "wordlist_base = [word for word in wordlist_base if len(word) > 4]\n",
    "wordlist_top = wordlist_base[:10]\n",
    "wordlist_random = random.sample(wordlist_base[10:40], 5)\n",
    "wordlist = wordlist_top + wordlist_random\n",
    "wordlist = [[word, None] for word in wordlist] # Needs to be in this format; can't remember why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordsearch import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puzzle = Crossword(20, 20, '-', 5000, wordlist)\n",
    "puzzle.compute_crossword(5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(puzzle.word_bank())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(puzzle.word_find())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(puzzle.solution())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "\n",
    "- ***Change collection(s)***: What authors or texts other than Catullus would you (or your students!) want to make word search puzzles for? Remember that you will need to use both a set of \"figure\" texts from which to make the puzzle *and* some collection of \"ground\" texts from which to measure keyness. For example, if you wanted to make a word search puzzle for Virgil's *Aeneid*, you might use the *Aeneid* as your figure text and then the entire collection of Latin epic as your ground text.\n",
    "- ***Change puzzle***: Word search puzzle can be fun, but try to think of other activities, puzzles, games, etc. that work with lists of words and especially from sets of \"key\" words. We can even move away from the world of \"play\" and consider how keyness could be useful to us as Latin teachers for, say, scaffolding vocabulary in a pre-reading activity.\n",
    "\n",
    "### For the future\n",
    "\n",
    "- ***Work better at scale*** We have gone about deriving TF-IDF in a manual fashion here—and for good reason as we want to make sure that we grasp the underlying concept of keyness before ramping things up. But there are much more efficient ways of calculating such measures for large amounts of text. For example, the `TfidfVectorizer` in the `scikit-learn` package can handle much of the overhead with much more flexibility in how we handle minimum counts, stopwords, etc. Here is a sample [tutorial](https://medium.com/@cmukesh8688/tf-idf-vectorizer-scikit-learn-dbc0244a911a). I have given an example of a count vectorizer and a tf-idf vectorizer dataframe for Catullus in an appendix below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "- Luca, D. 2018. Hebdomada Aenigmatum. Les premiers mots croisés en Latin et Grec. Paris: Dictionnaire."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Vectorized Catullus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "poem_nos, texts = zip(*catullus_preprocess.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV = CountVectorizer()\n",
    "CV_matrix = CV.fit_transform(texts)\n",
    "vocab = CV.get_feature_names_out()\n",
    "df_counts = pd.DataFrame(CV_matrix.toarray(), columns=vocab, index=poem_nos)\n",
    "df_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts.iloc[0].sort_values(ascending=False).where(lambda x: x > 0).dropna().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfvectorizer = TfidfVectorizer()\n",
    "tfidf_wm = tfidfvectorizer.fit_transform(texts)\n",
    "tfidf_tokens = tfidfvectorizer.get_feature_names_out()\n",
    "df_tfidfvect = pd.DataFrame(data = tfidf_wm.toarray(),index = poem_nos, columns = tfidf_tokens)\n",
    "df_tfidfvect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidfvect[['libellum']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidfvect[['passer']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidfvect.iloc[2].sort_values(ascending=False)[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit ('exploratory-philology')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "90f29fce041a746dff5b4e7dc43fcbd6facb181eb5d3b96918a0244a9c00c3c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
